{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Load and preprocess data\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from prep_data import preprocess_data_3\n",
    "\n",
    "seed = 2023\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_data, test_data = preprocess_data_3(17, 1, 16, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the loss function and the activation function are not leading to vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/265............. Loss: 207.1676\n",
      "Epoch: 10/265............. Loss: 158.9786\n",
      "Epoch: 15/265............. Loss: 128.6552\n",
      "Epoch: 20/265............. Loss: 107.4925\n",
      "Epoch: 25/265............. Loss: 91.4171\n",
      "Epoch: 30/265............. Loss: 79.3877\n",
      "Epoch: 35/265............. Loss: 72.0446\n",
      "Epoch: 40/265............. Loss: 75.2239\n",
      "Epoch: 45/265............. Loss: 69.1583\n",
      "Epoch: 50/265............. Loss: 59.3399\n",
      "Epoch: 55/265............. Loss: 53.6087\n",
      "Epoch: 60/265............. Loss: 44.4152\n",
      "Epoch: 65/265............. Loss: 35.0096\n",
      "Epoch: 70/265............. Loss: 41.5598\n",
      "Epoch: 75/265............. Loss: 29.8573\n",
      "Epoch: 80/265............. Loss: 48.7866\n",
      "Epoch: 85/265............. Loss: 22.5969\n",
      "Epoch: 90/265............. Loss: 21.2739\n",
      "Epoch: 95/265............. Loss: 27.2620\n",
      "Epoch: 100/265............. Loss: 15.0485\n",
      "Epoch: 105/265............. Loss: 22.8979\n",
      "Epoch: 110/265............. Loss: 38.4442\n",
      "Epoch: 115/265............. Loss: 14.1057\n",
      "Epoch: 120/265............. Loss: 32.0949\n",
      "Epoch: 125/265............. Loss: 11.9217\n",
      "Epoch: 130/265............. Loss: 10.8788\n",
      "Epoch: 135/265............. Loss: 8.5866\n",
      "Epoch: 140/265............. Loss: 24.5163\n",
      "Epoch: 145/265............. Loss: 8.0008\n",
      "Epoch: 150/265............. Loss: 27.6729\n",
      "Epoch: 155/265............. Loss: 7.9669\n",
      "Epoch: 160/265............. Loss: 6.4159\n",
      "Epoch: 165/265............. Loss: 5.7295\n",
      "Epoch: 170/265............. Loss: 5.4275\n",
      "Epoch: 175/265............. Loss: 5.1367\n",
      "Epoch: 180/265............. Loss: 4.8692\n",
      "Epoch: 185/265............. Loss: 4.6511\n",
      "Epoch: 190/265............. Loss: 4.4515\n",
      "Epoch: 195/265............. Loss: 4.2752\n",
      "Epoch: 200/265............. Loss: 4.1187\n",
      "Epoch: 205/265............. Loss: 3.9741\n",
      "Epoch: 210/265............. Loss: 3.8383\n",
      "Epoch: 215/265............. Loss: 3.7094\n",
      "Epoch: 220/265............. Loss: 3.5899\n",
      "Epoch: 225/265............. Loss: 3.4790\n",
      "Epoch: 230/265............. Loss: 3.3740\n",
      "Epoch: 235/265............. Loss: 3.2729\n",
      "Epoch: 240/265............. Loss: 3.1765\n",
      "Epoch: 245/265............. Loss: 3.0791\n",
      "Epoch: 250/265............. Loss: 2.9830\n",
      "Epoch: 255/265............. Loss: 2.8814\n",
      "Epoch: 260/265............. Loss: 2.7821\n",
      "Epoch: 265/265............. Loss: 2.7215\n"
     ]
    }
   ],
   "source": [
    "seed = 2023\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def cuda_get_device():\n",
    "\tis_cuda = torch.cuda.is_available()\n",
    "\tif is_cuda:\n",
    "\t\treturn torch.device(\"cuda\")\n",
    "\telse:\n",
    "\t\tprint(\"GPU not available, CPU used\")\n",
    "\t\treturn torch.device(\"cpu\")\n",
    "\n",
    "device = cuda_get_device()\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "\tdef __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "\t\tsuper(GRUModel, self).__init__()\n",
    "\n",
    "\t\t# Defining some parameters\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.n_layers = n_layers\n",
    "\n",
    "\t\t#Defining the layers\n",
    "\t\tself.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim)\n",
    "\t\t# May need a separate fc layer for each GRU unit?\n",
    "\t\tself.fc = nn.Linear(hidden_dim, output_size)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# Passing in the input into the GRU layer\n",
    "\t\thidden = self.gru(x)[0]\n",
    "\t\t\n",
    "\t\t# Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "\t\thidden = hidden.contiguous().view(-1, self.hidden_dim)\n",
    "\t\tout = self.fc(hidden)\n",
    "\t\t\n",
    "\t\treturn out\n",
    "\n",
    "# Model Hyperparameters\n",
    "hidden_state_size = 12\n",
    "fc_layers = 1\n",
    "# Training hyperparameters\n",
    "n_epochs = 265\n",
    "lr=0.001\n",
    "\n",
    "# Instantiate the model with hyperparameters\n",
    "model = GRUModel(input_size=64, output_size=1, hidden_dim=hidden_state_size, n_layers=fc_layers)\n",
    "model.to(device)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "# We may want to increase gamma so that the number of epochs can be reduced\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "milestones = [150, 300]\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.2)\n",
    "\n",
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\tepoch_losses = torch.tensor([0],dtype=float).to(device)\n",
    "\tfor data in train_data:\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tinput_data = torch.from_numpy(data[0]).float().to(device)\n",
    "\t\ttarget_pred = torch.tensor(data[1]).unsqueeze(0).float().to(device)\n",
    "\t\toutput = model(input_data)[-1]\n",
    "\t\t# print(torch.log(torch.sigmoid(output)).item(), torch.log(1 - torch.sigmoid(output)).item(), target_pred.item())\n",
    "\t\tloss = criterion(output, target_pred)\n",
    "\t\t# print(loss)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tepoch_losses += loss\n",
    "\tscheduler.step()\n",
    "\tif epoch%5 == 0:\n",
    "\t\tprint('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "\t\tprint(\"Loss: {:.4f}\".format(epoch_losses.to('cpu').item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "290\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "\tfor data in train_data:\n",
    "\t\t\tinput_data = torch.from_numpy(data[0]).float().to(device)\n",
    "\t\t\ttarget_pred = torch.tensor(data[1]).unsqueeze(0)\n",
    "\t\t\toutput = torch.sigmoid(model(input_data)[-1])\n",
    "\t\t\tpredictions.append(output.cpu().item())\n",
    "\t\t\ttargets.append(target_pred.item())\n",
    "\n",
    "pred = np.array(predictions)\n",
    "pred = np.rint(pred).astype(int)\n",
    "targets_numpy = np.array(targets)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(pred)\n",
    "# plt.plot(targets_numpy)\n",
    "# plt.show()\n",
    "\n",
    "print(np.sum(np.abs(targets_numpy-pred)))\n",
    "print(np.sum(targets_numpy))\n",
    "print(np.sum(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "10\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "\tfor data in test_data:\n",
    "\t\t\tinput_data = torch.from_numpy(data[0]).float().to(device)\n",
    "\t\t\ttarget_pred = torch.tensor(data[1]).unsqueeze(0)\n",
    "\t\t\toutput = torch.sigmoid(model(input_data)[-1])\n",
    "\t\t\tpredictions.append(output.cpu().item())\n",
    "\t\t\ttargets.append(target_pred.item())\n",
    "\n",
    "pred = np.array(predictions)\n",
    "pred = np.rint(pred).astype(int)\n",
    "targets_numpy = np.array(targets)\n",
    "\n",
    "print(np.sum(pred))\n",
    "print(np.sum(np.abs(targets_numpy-pred)))\n",
    "print(np.sum(targets_numpy))\n",
    "\n",
    "# print(np.where(test_targets != pred))\n",
    "# print(np.where(np.abs(test_targets-pred) != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
