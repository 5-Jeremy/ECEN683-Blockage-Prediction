{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload files when they are changed so there is no need to restart the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For plotting the loss curve\n",
    "%matplotlib widget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# This is the seed that I used to get the reported accuracy levels\n",
    "seed = 2023\n",
    "def reset_seed():\n",
    "\ttorch.manual_seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "def cuda_get_device():\n",
    "\tis_cuda = torch.cuda.is_available()\n",
    "\tif is_cuda:\n",
    "\t\treturn torch.device(\"cuda\")\n",
    "\telse:\n",
    "\t\tprint(\"GPU not available, CPU used\")\n",
    "\t\treturn torch.device(\"cpu\")\n",
    "\n",
    "# Define the model class\n",
    "class GRUWithConvLayer(nn.Module):\n",
    "\tdef __init__(self, input_size, output_size, hidden_dim, conv_dropout, fc_dropout):\n",
    "\t\tsuper(GRUWithConvLayer, self).__init__()\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.convlayer = nn.Conv2d(1, 1, (5, 5), stride=1, padding=0)\n",
    "\t\tself.batchnorm = nn.BatchNorm2d(1)\n",
    "\t\tself.conv_dropout = nn.Dropout(p=conv_dropout)\n",
    "\t\tself.gru = nn.GRU(input_size=input_size - 4, hidden_size=hidden_dim, batch_first=True)\n",
    "\t\tself.fc_dropout = nn.Dropout(p=fc_dropout)\n",
    "\t\tself.fc1= nn.Linear(hidden_dim, 100)\n",
    "\t\tself.fc2 = nn.Linear(100, output_size)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tafter_conv = self.batchnorm(self.convlayer(x)).squeeze(1)\n",
    "\t\tafter_conv_drop = self.conv_dropout(after_conv)\n",
    "\t\thidden = self.gru(after_conv_drop)[0]\n",
    "\t\tout = self.fc1(hidden)\n",
    "\t\tout = self.fc_dropout(out)\n",
    "\t\tout = self.fc2(out)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55982\n",
      "6226\n"
     ]
    }
   ],
   "source": [
    "from prep_data import preprocess_data_main\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# The number of samples following the last observed sample for which the model must predict the blockage status\n",
    "pred_length = 10\n",
    "# The number of mmWave samples used to make predictions; changing this will change the number of sequences available for training and testing\n",
    "seq_length = 10\n",
    "# The ratio of training to testing data; this proportion of the set of available sequences will be used for training\n",
    "train_ratio = 0.9\n",
    "# The number of sequences in each batch\n",
    "batch_size = 2000\n",
    "# Whether or not to augment all of the training sequences by reversing the order of the beams; this has been shown to reduce performance\n",
    "\t# for any prediction length, so it is not recommended\n",
    "augment = False\n",
    "# Whether or not to include the center beams in the observation data\n",
    "remove_center = False\n",
    "# If the center beams are included, whether to normalize the center and non-center beams jointly or separately\n",
    "joint_normalize = True\n",
    "# Whether or not to shuffle the training data before batching; if this is false, the same samples will be batched together every time\n",
    "\t# the data is loaded\n",
    "shuffle = True\n",
    "\n",
    "train_data, test_data, train_len, test_len = preprocess_data_main(pred_length, seq_length, train_ratio, batch_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  augment=augment, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  remove_center=remove_center,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  joint_normalize=joint_normalize,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  shuffle=shuffle)\n",
    "\n",
    "print(train_len)\n",
    "print(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/2600............. Loss: 0.6850\n",
      "Epoch: 200/2600............. Loss: 0.6706\n",
      "Epoch: 300/2600............. Loss: 0.6575\n",
      "Epoch: 400/2600............. Loss: 0.6521\n",
      "Epoch: 500/2600............. Loss: 0.6435\n",
      "Epoch: 600/2600............. Loss: 0.6420\n",
      "Epoch: 700/2600............. Loss: 0.6383\n",
      "Epoch: 800/2600............. Loss: 0.6438\n",
      "Epoch: 900/2600............. Loss: 0.6391\n",
      "Epoch: 1000/2600............. Loss: 0.6374\n",
      "Epoch: 1100/2600............. Loss: 0.6364\n",
      "Epoch: 1200/2600............. Loss: 0.6326\n",
      "Epoch: 1300/2600............. Loss: 0.6343\n",
      "Epoch: 1400/2600............. Loss: 0.6338\n",
      "Epoch: 1500/2600............. Loss: 0.6357\n",
      "Epoch: 1600/2600............. Loss: 0.6306\n",
      "Epoch: 1700/2600............. Loss: 0.6254\n",
      "Epoch: 1800/2600............. Loss: 0.6244\n",
      "Epoch: 1900/2600............. Loss: 0.6261\n",
      "Epoch: 2000/2600............. Loss: 0.6276\n",
      "Epoch: 2100/2600............. Loss: 0.6254\n",
      "Epoch: 2200/2600............. Loss: 0.6246\n",
      "Epoch: 2300/2600............. Loss: 0.6275\n",
      "Epoch: 2400/2600............. Loss: 0.6239\n",
      "Epoch: 2500/2600............. Loss: 0.6233\n",
      "Epoch: 2600/2600............. Loss: 0.6205\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "device = cuda_get_device()\n",
    "\n",
    "### Model Hyperparameters\n",
    "# The length of the hidden state vector used in the GRU\n",
    "hidden_state_size = 20\n",
    "# The dropout probability between the convolutional layer and the GRU\n",
    "conv_dropout = 0.4\n",
    "# The dropout probability between the two fully connected layers\n",
    "fc_dropout = 0.4\n",
    "# The weight applied to the loss for samples with a positive label; making this > 1 helps the model to learn patterns that accompany blockages\n",
    "pos_weight = 1.4\n",
    "# The learning rate\n",
    "lr = 0.03\n",
    "# The factor by which the learning rate is reduced at each milestone\n",
    "gamma = 0.6\n",
    "# The total number of epochs for training\n",
    "n_epochs = 2600\n",
    "# The milestones at which the learning rate is reduced\n",
    "milestones =  [100, 200, 400, 1000, 1500, 1600, 1700]\n",
    "# Whether or not to plot the loss curve\n",
    "plot_losses = False\n",
    "\n",
    "if remove_center:\n",
    "\tinput_size = 54\n",
    "else:\n",
    "\tinput_size = 64\n",
    "\n",
    "model = GRUWithConvLayer(input_size=input_size, output_size=1, hidden_dim=hidden_state_size, conv_dropout=conv_dropout, fc_dropout=fc_dropout)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# Training Run\n",
    "loss_sequence = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\tepoch_losses = torch.tensor([0],dtype=float).to(device)\n",
    "\tfor data in train_data:\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Need to include an additional dimension of size 1 for the number of channels\n",
    "\t\tinput_data = data[0].unsqueeze(1).float().to(device)\n",
    "\t\ttarget_pred = data[1].unsqueeze(1).float().to(device)\n",
    "\t\toutput = model(input_data)[:,-1,:]\n",
    "\t\tloss = criterion(output, target_pred)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tepoch_losses += loss\n",
    "\tscheduler.step()\n",
    "\tavg_epoch_loss = epoch_losses.to('cpu').item()/len(train_data)\n",
    "\tloss_sequence.append(avg_epoch_loss)\n",
    "\tif epoch%100 == 0:\n",
    "\t\tprint('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "\t\tprint(\"Loss: {:.4f}\".format(avg_epoch_loss))\n",
    "\n",
    "# Plot the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "if plot_losses:\n",
    "\tplt.close('all')\n",
    "\tplt.figure()\n",
    "\tplt.plot(loss_sequence)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks missed: 1912\n",
      "Number of false positives: 425\n",
      "Accuracy: 0.6246386122711212\n"
     ]
    }
   ],
   "source": [
    "# Get the test accuracy\n",
    "reset_seed()\n",
    "model.eval()\n",
    "\n",
    "num_blocks_missed = 0\n",
    "num_false_positives = 0\n",
    "with torch.no_grad():\n",
    "\tfor data in test_data:\n",
    "\t\tinput_data = data[0].unsqueeze(1).float().to(device)\n",
    "\t\ttarget_pred = data[1].unsqueeze(1)\n",
    "\t\toutput = torch.round(torch.sigmoid(model(input_data)[:,-1,:])).cpu()\n",
    "\t\terr = (output - target_pred).numpy()\n",
    "\t\tnum_blocks_missed += np.sum(err < 0)\n",
    "\t\tnum_false_positives += np.sum(err > 0)\n",
    "\t\n",
    "print(\"Number of blocks missed: {}\".format(num_blocks_missed))\n",
    "print(\"Number of false positives: {}\".format(num_false_positives))\n",
    "print(\"Accuracy: {}\".format(1 - (num_blocks_missed + num_false_positives)/test_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
