{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload files when they are changed so there is no need to restart the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# For plotting the loss curve\n",
    "%matplotlib widget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# This is the seed that I used to get the reported accuracy levels\n",
    "seed = 2023\n",
    "def reset_seed():\n",
    "\ttorch.manual_seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "def cuda_get_device():\n",
    "\tis_cuda = torch.cuda.is_available()\n",
    "\tif is_cuda:\n",
    "\t\treturn torch.device(\"cuda\")\n",
    "\telse:\n",
    "\t\tprint(\"GPU not available, CPU used\")\n",
    "\t\treturn torch.device(\"cpu\")\n",
    "\n",
    "# Define the model class\n",
    "class GRUWithConvLayer(nn.Module):\n",
    "\tdef __init__(self, input_size, output_size, hidden_dim, conv_dropout, fc_dropout):\n",
    "\t\tsuper(GRUWithConvLayer, self).__init__()\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.convlayer = nn.Conv2d(1, 1, (5, 5), stride=1, padding=0)\n",
    "\t\tself.batchnorm = nn.BatchNorm2d(1)\n",
    "\t\tself.conv_dropout = nn.Dropout(p=conv_dropout)\n",
    "\t\tself.gru = nn.GRU(input_size=input_size - 4, hidden_size=hidden_dim, batch_first=True)\n",
    "\t\tself.fc_dropout = nn.Dropout(p=fc_dropout)\n",
    "\t\tself.fc1= nn.Linear(hidden_dim, 100)\n",
    "\t\tself.fc2 = nn.Linear(100, output_size)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tafter_conv = self.batchnorm(self.convlayer(x)).squeeze(1)\n",
    "\t\tafter_conv_drop = self.conv_dropout(after_conv)\n",
    "\t\thidden = self.gru(after_conv_drop)[0]\n",
    "\t\tout = self.fc1(hidden)\n",
    "\t\tout = self.fc_dropout(out)\n",
    "\t\tout = self.fc2(out)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_data import preprocess_data_main, boolean_to_letter\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# The number of samples following the last observed sample for which the model must predict the blockage status\n",
    "pred_length = 10\n",
    "# The number of mmWave samples used to make predictions; changing this will change the number of sequences available for training and testing\n",
    "seq_length = 10\n",
    "# The ratio of training to testing data; this proportion of the set of available sequences will be used for training\n",
    "train_ratio = 0.9\n",
    "# The number of sequences in each batch\n",
    "batch_size = 2000\n",
    "# Whether or not to augment all of the training sequences by reversing the order of the beams; this has been shown to reduce performance\n",
    "\t# for any prediction length, so it is not recommended\n",
    "augment = False\n",
    "# Whether or not to include the center beams in the observation data\n",
    "remove_center = False\n",
    "# If the center beams are included, whether to normalize the center and non-center beams jointly or separately\n",
    "joint_normalize = True\n",
    "# Whether or not to shuffle the training data before batching; if this is false, the same samples will be batched together every time\n",
    "\t# the data is loaded\n",
    "shuffle = True\n",
    "\n",
    "# If this is enabled, the code will attempt to load the training and testing data from a file with the exact same parameters\n",
    "\t# If the desired data configuration has not yet been saved to a file, the data will be generated and saved to a file\n",
    "\t# The filename is based on the parameters selected above\n",
    "use_saved_data = True\n",
    "\n",
    "if use_saved_data:\n",
    "\tfilename = \"Preprocessed Data\\\\Prediction Length {}\\\\{}_{}_{}_{}_{}_{}_{}.p\".format(pred_length, seq_length, train_ratio, batch_size, boolean_to_letter(augment),boolean_to_letter(remove_center),boolean_to_letter(joint_normalize),boolean_to_letter(shuffle))\n",
    "\ttry: \n",
    "\t\topen(filename, \"rb\")\n",
    "\t\ttrain_data, test_data, train_len, test_len = pickle.load(open(filename, \"rb\"))\n",
    "\texcept FileNotFoundError:\n",
    "\t\ttrain_data, test_data, train_len, test_len = preprocess_data_main(pred_length, seq_length, train_ratio, batch_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taugment=augment, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tremove_center=remove_center,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tjoint_normalize=joint_normalize,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshuffle=shuffle)\n",
    "\t\tpickle.dump((train_data, test_data, train_len, test_len), open(filename, \"wb+\"))\n",
    "else:\n",
    "\ttrain_data, test_data, train_len, test_len = preprocess_data_main(pred_length, seq_length, train_ratio, batch_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taugment=augment, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tremove_center=remove_center,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tjoint_normalize=joint_normalize,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshuffle=shuffle)\n",
    "\n",
    "print(train_len)\n",
    "print(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a fresh model, run this cell. To load a saved model, run the cell at the end of this notebook and then run the cell below this one to get the accuracy.\n",
    "reset_seed()\n",
    "device = cuda_get_device()\n",
    "\n",
    "### Model Hyperparameters\n",
    "# The length of the hidden state vector used in the GRU\n",
    "hidden_state_size = 20\n",
    "# The dropout probability between the convolutional layer and the GRU\n",
    "conv_dropout = 0.4\n",
    "# The dropout probability between the two fully connected layers\n",
    "fc_dropout = 0.4\n",
    "# The weight applied to the loss for samples with a positive label; making this > 1 helps the model to learn patterns that accompany blockages\n",
    "pos_weight = 1.4\n",
    "# The learning rate\n",
    "lr = 0.03\n",
    "# The factor by which the learning rate is reduced at each milestone\n",
    "gamma = 0.6\n",
    "# The total number of epochs for training\n",
    "n_epochs = 2600\n",
    "# The milestones at which the learning rate is reduced\n",
    "milestones =  [100, 200, 400, 1000, 1500, 1600, 1700]\n",
    "# Whether or not to plot the loss curve\n",
    "plot_losses = False\n",
    "\n",
    "if remove_center:\n",
    "\tinput_size = 54\n",
    "else:\n",
    "\tinput_size = 64\n",
    "\n",
    "model = GRUWithConvLayer(input_size=input_size, output_size=1, hidden_dim=hidden_state_size, conv_dropout=conv_dropout, fc_dropout=fc_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# Whether to load pre-trained parameters to train a fresh model; in order to load a pre-trained model, it must have the same settings for\n",
    "\t# pred_length, input_size and hidden_state_size\n",
    "load_existing_parameters = True\n",
    "\n",
    "if load_existing_parameters:\n",
    "\tparameter_filename = \"Saved Models\\\\Prediction Length {}\\\\{}_{}.model\".format(pred_length, input_size, hidden_state_size)\n",
    "\tmodel.load_state_dict(torch.load(parameter_filename))\n",
    "else:\n",
    "\tcriterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\tscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "\t# Training Run\n",
    "\tloss_sequence = []\n",
    "\tfor epoch in range(1, n_epochs + 1):\n",
    "\t\tepoch_losses = torch.tensor([0],dtype=float).to(device)\n",
    "\t\tfor data in train_data:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\t# Need to include an additional dimension of size 1 for the number of channels\n",
    "\t\t\tinput_data = data[0].unsqueeze(1).float().to(device)\n",
    "\t\t\ttarget_pred = data[1].unsqueeze(1).float().to(device)\n",
    "\t\t\toutput = model(input_data)[:,-1,:]\n",
    "\t\t\tloss = criterion(output, target_pred)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tepoch_losses += loss\n",
    "\t\tscheduler.step()\n",
    "\t\tavg_epoch_loss = epoch_losses.to('cpu').item()/len(train_data)\n",
    "\t\tloss_sequence.append(avg_epoch_loss)\n",
    "\t\tif epoch%100 == 0:\n",
    "\t\t\tprint('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "\t\t\tprint(\"Loss: {:.4f}\".format(avg_epoch_loss))\n",
    "\n",
    "\t# Plot the loss curve\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tif plot_losses:\n",
    "\t\tplt.close('all')\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(loss_sequence)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t# Save the model parameters; if the base directory does not exist, it will be created\n",
    "\tif not os.path.exists(\"Saved Models\\\\Prediction Length {}\".format(pred_length)):\n",
    "\t\tos.makedirs(\"Saved Models\\\\Prediction Length {}\".format(pred_length))\n",
    "\ttorch.save(model.state_dict(),\"Saved Models\\\\Prediction Length {}\\\\{}_{}.model\".format(pred_length, input_size, hidden_state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test accuracy\n",
    "reset_seed()\n",
    "model.eval()\n",
    "\n",
    "num_blocks_missed = 0\n",
    "num_false_positives = 0\n",
    "with torch.no_grad():\n",
    "\tfor data in test_data:\n",
    "\t\tinput_data = data[0].unsqueeze(1).float().to(device)\n",
    "\t\ttarget_pred = data[1].unsqueeze(1)\n",
    "\t\toutput = torch.round(torch.sigmoid(model(input_data)[:,-1,:])).cpu()\n",
    "\t\terr = (output - target_pred).numpy()\n",
    "\t\tnum_blocks_missed += np.sum(err < 0)\n",
    "\t\tnum_false_positives += np.sum(err > 0)\n",
    "\t\n",
    "print(\"Number of blocks missed: {}\".format(num_blocks_missed))\n",
    "print(\"Number of false positives: {}\".format(num_false_positives))\n",
    "print(\"Accuracy: {}\".format(1 - (num_blocks_missed + num_false_positives)/test_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
